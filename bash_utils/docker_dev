#!/bin/bash
set -e

#******************************************************************************
# Copyright (c) 2024 Zipline International, Inc.  All rights reserved.
#
# docker_dev creates a persistent docker container for user development. This is
# meant to enable us to use development environments generated by CI while
# isolating FlightSystems dependencies from the host filesystem. This operates
# similarly to docker_bash, but the container is persistent and therefore
# reentrant.
#
# On Linux systems, docker uses no virtualization, so the only overhead cost of
# working within this system should mostly come from the layered filesystem.
# This concept should extend cleanly to MacOS (with some changes to support zsh and
# different device paths), but will incur nontrivial cost from virtualization.
#
# This script adds a base layer on top of the upstream docker images used by
# docker_do, adding some conveniences into the filesystem and allowing users to
# optionally define their own layers (see dockerfule.user.example) and docker
# arguments (see user_config.json.example).
#
# Should this have just been done with docker compose?
#
# ...probably.
#
# Should this also have been written in Python?
#
# Yuuuuuuup.
#===============================================================================


############ Set paths ############

SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
FILES_DIR="${SCRIPT_DIR}"/docker_dev_files
CONFIG_FILE="${FILES_DIR}"/user_config.json


############ Sanity check environment ############

if [ -n "${REPO_PATH}" ]; then
    source "${REPO_PATH}"/sim/utils/utils.sh
else
    printf "ERROR: path to repository root is not defined.\n"
    printf "ERROR: please execute: source <repo root>/docker_fun.sh\n"
    exit 1
fi

# This script should not be run within a docker container.
if [ -e "/proc/1/sched" ] && cat /proc/1/sched | head -n 1 | \grep -q "bash"; then
    exit 1
fi


############ Define common docker create args ############

# Configure container with lenient default args for development. We really just want
# filesystem isolation, so make everything privileged.

# These arguments should be common among all developers. For arguments specific to one
# machine, see user_config.json.example.

docker_create_common_flags=(
    --privileged
    --user "$(id -u):$(id -g)"
    --workdir "${REPO_PATH}"
    --network=host
    -it
    --entrypoint /bin/bash
)

docker_create_common_mounts=(
    -v "${HOME}"/.cache:"${HOME}"/.cache
    -v "${HOME}"/.aws:"${HOME}"/.aws
    -v "${HOME}"/.ssh:"${HOME}"/.ssh
    -v "${HOME}"/.Xauthority:/root/.Xauthority:ro
    -v "${REPO_PATH}":"${REPO_PATH}"
    -v /dev:/dev
    -v /run:/run
    -v /var/run/docker.sock:/var/run/docker.sock
    -v /tmp/.X11-unix:/tmp/.X11-unix:rw
)

docker_create_common_devices=(
    --device /dev/dri
)

docker_create_common_env_vars=(
    -e QT_X11_NO_MITSHM=1
    -e DISPLAY="${DISPLAY}"
)

############ Helper functions ############

# Dump usage information and exit.
function print_usage_and_exit() {
    echo "
Usage: docker_dev [options]

Options:
  run                   (Default) Spin up and enter the development environment.
  start                 Spin up a development environment, but don't enter the container.
  stop                  Stop the running container.
  delete                Delete the running container and user image.
  update                Pull the latest image and rebuild.
  configure_code        Configure VS Code for containerized development.
"
    exit 0
}

# Install specified apt packages if missing
function maybe_apt_install() {
    for package in "$@"; do
        if ! dpkg -s "$package" > /dev/null 2>&1; then
            print_info "Installing $package"
            sudo apt install "$package" -y
        fi
    done
}

# Check if the user wants to disable the cuda image.
function disable_cuda() {
    if [ -f "$CONFIG_FILE" ]; then
        jq -e '.no_cuda == "true"' < "${CONFIG_FILE}" > /dev/null
    else
        return 1
    fi
}

# Check if the user wants to use the cuda image regardless of whether or not
# the nvidia runtime is used.
function with_cuda_image() {
    with_nvidia && ! disable_cuda
}

# Get the base image tag.
function base_image_tag() {
    if with_cuda_image; then
        echo "zipline/flight-systems-cuda:latest"
    else
        echo "zipline/flight-systems-build:latest"
    fi
}

# Get the container name.
function container_name() {
    if with_cuda_image; then
        echo "perception-cuda-dev"
    else
        echo "perception-dev"
    fi
}

# Get the user image tag. This is the image we'll build on top of the base
# image and tag specifically for the developer.
function user_image_tag() {
    echo "$(base_image_tag)-${USER}" | jq -rR 'ascii_downcase'
}

# Run any missing configuration required on Nvidia systems.
function maybe_install_nvidia_runtime() {
    # We'll use the nvidia container runtime to handle forwarding all cuda-specific
    # capabilities and nvidia drivers support for us.
    if [[ ! $(type -P nvidia-container-runtime) ]]; then
        print_warning "Nvidia container runtime not found."
        read -r -e -p " Install and configure? [Y/n]: " REPLY

        if [[ ${REPLY} == [Nn]*   ]]; then
            print_error "Configuration failed."
            exit 2
        fi

        # Configure the apt repository.
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
            sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && \
        curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list > /dev/null
        # Now install the runtime.
        sudo apt update
        sudo apt install -y nvidia-container-runtime

        # Configure the docker daemon so we can optionally select the nvidia runtime.
        sudo tee /etc/docker/daemon.json > /dev/null <<EOF
{
    "runtimes": {
        "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
        }
    }
}
EOF
        # Restart the docker service so the runtime is available as an option.
        sudo systemctl reload docker.service
        print_info "Nvidia runtime configured."
    fi
}

# Check if the host is running nvidia drivers.
function with_nvidia() {
    # Use the presence of nvidia-smi as an indicator that we're running on an Nvidia system. This can cause
    # things to break on systems with Nvidia cards, but where prime-select indicates that another adaptor
    # is being used.
    if [[ $(type -P nvidia-smi)  ]]; then
        return 0
    else 
        return 1
    fi
}

# Set up Nvidia-specific things.
function configure_nvidia() {
    # Specify that we should use the nvidia runtime.
    docker_create_common_flags+=(--runtime=nvidia)

    # Also install the runtime if we don't have it.
    maybe_install_nvidia_runtime
}

# Pull the base image that we'll build on top of.
function pull_base_image() {
    if with_cuda_image; then
        docker_pull -c
    else
        docker_pull
    fi
}

# If we don't have the user image, build it here.
function maybe_build_user_image() {

    if [[ $(docker images -q "$(base_image_tag)" 2> /dev/null) == "" ]]; then
        print_info "Base image $(base_image_tag) not found in local repository. Pulling now."
        pull_base_image
    fi

    # If the image doesn't exist...
    if [[ $(docker images -q "$(user_image_tag)" 2> /dev/null) == "" ]]; then
        # This introduces new layers on top of the base image provided by CI and
        # bakes in the user and group and some graphics libraries.
        print_info "User dev image not found. Building now."
        docker build \
            --build-arg BASE_IMAGE="$(base_image_tag)" \
            --build-arg USER="${USER}" \
            --build-arg UID="${UID}" \
            --build-arg GID="$(id --group)" \
            -t "$(user_image_tag)" \
            -f "${FILES_DIR}/dockerfile.base" \
            "${FILES_DIR}"

        # If there is a user-defined dockerfile, build a new image by appending that
        # dockerfile on top of the user base.
        if [ -f "${FILES_DIR}/dockerfile.user" ]; then
            print_info "Custom dockerfile found. Creating custom image."

            # Retag the image we just built as a base.
            docker tag "$(user_image_tag)" "$(user_image_tag)"-base

            # Now build the user layer.
            zipline_pip_url=$(grep -Po "(?<=index-url = ).+(?=$)" "$HOME/.pip/pip.conf")
            docker build \
                --build-arg BASE_IMAGE="$(user_image_tag)"-base \
                --build-arg ZIPLINE_PIP_URL="$zipline_pip_url" \
                -t "$(user_image_tag)" \
                -f "${FILES_DIR}/dockerfile.user" \
                "${FILES_DIR}"

            # Delete the intermediate layer. This should not affect the cache,
            # but is just meant to make things less confusing to the user.
            docker image rm "$(user_image_tag)"-base
        fi
    fi
}

# Create the container if needed.
function maybe_create_dev_container() {
    # If we can't find a running container...
    if [[ $(docker ps -a -q -f name="$(container_name)" 2> /dev/null) == "" ]]; then
        # Allow a user to specify additional arguments through the user_config.json file,
        # if it exists.
        if [ -f "$CONFIG_FILE" ]; then
            print_info "Loading user-specified docker arguments."

            docker_create_user_mounts=$(
                envsubst < "$CONFIG_FILE" | jq -r '.mounts | to_entries | .[] | "-v " + .key + ":" + .value'
            )

            docker_create_user_env_vars=$(
                envsubst < "$CONFIG_FILE" | jq -r '.env_vars | to_entries | .[] | "-e " + .key + "=" + .value'
            )

            docker_create_user_devices=$(
                envsubst < "$CONFIG_FILE" | jq -r '.devices | .[] | "--device " + .'
            )

            docker_create_user_flags=$(
                envsubst < "$CONFIG_FILE" | jq -r '.additional_flags | .[]'
            )
        fi

        docker_create_args=(
            "${docker_create_common_mounts[@]}"
            "${docker_create_common_devices[@]}"
            "${docker_create_common_env_vars[@]}"
            "${docker_create_common_flags[@]}"
            "${docker_create_user_mounts}"
            "${docker_create_user_env_vars}"
            "${docker_create_user_devices}"
            "${docker_create_user_flags}"
        )

        print_info "No container found. Creating"
        docker create ${docker_create_args[@]} --name "$(container_name)" "$(user_image_tag)"
    fi
}

# Start the container.
function start_container() {
    # Start the container. No-op if already started.
    docker start "$(container_name)"
}

# Execute an interactive bash process within the container.
function enter_container () {
    docker exec -it "$(container_name)" "$SHELL"
}

# Install VS code extensions, if missing.
function install_vs_code_extensions() {
    for extension in "$@"; do
        if code --list-extensions | grep -q "$extension"; then
            print_info "Found VS Code extension: $extension"
        else
            print_info "Installing VS Code extension: $extension"
            code --install-extension "$extension"
        fi
    done
}

function write_vs_code_container_config() {
    # Create the config directory if it doesn't exist.
    CONFIG_DIR="$HOME"/.config/Code/User/globalStorage/ms-vscode-remote.remote-containers/imageConfigs
    mkdir -p "$CONFIG_DIR"

    # Now figure out where the configuration should end up.
    CONFIG_FILENAME=$(user_image_tag | jq -Rr '@uri | ascii_downcase')
    TARGET_CONFIG="$CONFIG_DIR/$CONFIG_FILENAME.json"

    SOURCE_CONFIG="$FILES_DIR"/code_container_config.json

    sed "s|WORKSPACE_FOLDER|${REPO_PATH}|g" "$SOURCE_CONFIG" > "$TARGET_CONFIG"
    print_info "Configuration written to $CONFIG_DIR/"
}

function configure_vs_code() {
    if ! command -v code &> /dev/null; then
        print_error "VS Code not found on path. Unable to configure."
        exit 1
    fi

    install_vs_code_extensions ms-vscode-remote.remote-containers
    write_vs_code_container_config
}


############ Install missing dependencies ############
maybe_apt_install jq


############ Parsing and execution ############

# Default run options for this script.
OPTION="run"

# Parse user input.
while (($# > 0)); do
    case $1 in
        -h | --help)
            print_usage_and_exit
            ;;
        run )
            # Default option. No special handling here.
            ;;
        configure_code )
            configure_vs_code
            exit 0
            ;;
        start | stop | delete | update)
            if [[ ${OPTION} != "run" ]]; then
                print_error "Only one option is supported."
                exit 2
            fi
            OPTION=${1}
            ;;
        *) print_error "Unrecognized argument: ${1}"
            print_usage_and_exit
            ;;
    esac
    shift
done

# Options requiring us to stop the container.
case "${OPTION}" in stop | delete | update)
    print_info "Killing development container."
    docker stop "$(container_name)" 2> /dev/null || true
esac

# Options requiring us to delete the container and image.
case ${OPTION} in delete | update)
    print_info "Removing development container."
    docker rm "$(container_name)" 2> /dev/null || true
    print_info "Removing user image."
    docker image rm "$(user_image_tag)" 2> /dev/null || true
esac

# Options requiring us to force pull
case $OPTION in update)
    # Force pull a new base image to build on. 
    pull_base_image 
esac

# Options requiring us to build/run a new container.
case $OPTION in start | update | run )
    # User just wants to attach to a container (that may or may not be created yet)
    # and do their thing.
    if with_nvidia; then
        configure_nvidia
    fi
    maybe_build_user_image
    maybe_create_dev_container
    start_container
esac

# Options requiring us to enter the container.
case $OPTION in run )
    enter_container
esac
